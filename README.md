# Stereo Depth Estimation and 3D Object Detection

This repository contains implementations for comparing HitNet and CREStereo algorithms for stereo depth estimation and 3D object detection.

## Overview

This project implements and compares two state-of-the-art stereo depth estimation methods: HitNet and CREStereo. The depth maps generated by these methods are used for 3D object detection, allowing for comprehensive scene understanding from stereo image pairs.

## Features

- Implementation of HitNet and CREStereo for stereo depth estimation
- 3D object detection based on depth maps
- Performance comparison between the algorithms
- Interactive visualization tools
- Video processing pipeline

## Installation

### Prerequisites

- CUDA-capable GPU (recommended)
- Python 3.9+
- Conda (optional but recommended)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/binhphan77373/Comparision_HitNet_and_CREStereo_Stereo_Depth_Estimation_3D_Object_Detection.git
cd Comparision_HitNet_and_CREStereo_Stereo_Depth_Estimation_3D_Object_Detection
```

2. Create and activate a conda environment:
```bash
conda create --name stereo_depth_env python=3.9
conda activate stereo_depth_env
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

If you encounter errors, install the following packages individually:
```bash
pip install PyQt5==5.15.9
pip install opencv-python
pip install torch torchvision torchaudio
pip install tqdm
pip install open3d
pip install ultralytics
pip install tensorflow
pip install flopth
pip install ptflops
```

Alternatively, you can create the environment directly from the provided YAML file:
```bash
conda env create -f environment.yml
```

## Dataset

This project uses the KITTI stereo dataset. The dataset structure should be organized as follows:

```
data_scene_flow_calib/
└── training/
    └── calib_cam_to_cam/*.txt

data_scene_flow/
├── training/
    ├── image_2/*_10.png    # Left camera images
    ├── image_3/*_10.png    # Right camera images
    └── disp_noc_0/*.png    # Ground truth disparity maps
```

Update the `config.py` file with your dataset paths:

```python
# Example config.py paths
KITTI_CALIB_FILES_PATH = "/home/eyecode-binh/Stereo_Depth_Estimation/Hitnet-stereo-depthEstimation_3Dobject-detection/data_scene_flow_calib/training/calib_cam_to_cam/*.txt"
KITTI_LEFT_IMAGES_PATH = "/home/eyecode-binh/Stereo_Depth_Estimation/Hitnet-stereo-depthEstimation_3Dobject-detection/data_scene_flow/training/image_2/*_10.png"
KITTI_RIGHT_IMAGES_PATH = "/home/eyecode-binh/Stereo_Depth_Estimation/Hitnet-stereo-depthEstimation_3Dobject-detection/data_scene_flow/training/image_3/*_10.png"
KITTI_DISP_PATH = "/home/eyecode-binh/Stereo_Depth_Estimation/Hitnet-stereo-depthEstimation_3Dobject-detection/data_scene_flow/training/disp_noc_0/*.png"
```

## Usage

### Running the Demo

To run the video processing demo:
```bash
python demo_video.py
```

## Results

The repository includes tools to compare the two stereo estimation methods across multiple metrics:
- Depth accuracy
- Processing speed
- Memory usage
- 3D object detection precision

Sample results and visualizations are stored in the `results/` directory.

## Citation

If you use this code in your research, please cite:

```
@article{hitnet2021,
  title={HitNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching},
  author={...},
  journal={...},
  year={2021}
}

@article{crestereo2022,
  title={Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation},
  author={...},
  journal={...},
  year={2022}
}

@article{Geiger2012CVPR,
  title={Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2012}
}
```

## License

[Specify the license under which this project is released]

## Acknowledgements

- KITTI Vision Benchmark Suite for providing the stereo dataset
